# Sensitivity analysis of large tokamak
# 8 uncertain parameters

# Dakota Input File
# Usage:
#   dakota -i sa_lt.in > sa_lt_summary.out

environment
  # Summary output
  # results_output
    # Doesn't output anything
    # results_output_file "summary"
  # full output data
  # output_file "sa_lt.out" # Contains result summary
  tabular_data
    tabular_data_file "sa_lt.dat"

# SA study
method
#  polynomial_chaos
#  sparse_grid_level = 2
#  variance_based_decomp 
#    interaction_order = 2

  sampling
    sample_type lhs
    samples = 1000 # preferably 1000s
    variance_based_decomp

variables
  active epistemic
  continuous_interval_uncertain = 8
    descriptors   "fimp_14" "ralpne" "kappa"  "flhthresh"  "boundu_2"  "cboot" "pinjalw"  "etath"
    lower_bounds  1.0e-5     0.085     1.8         0.85        11.0     0.95      51.0     0.36   
    upper_bounds  1.0e-4     0.115     1.9         1.15        12.0     1.05     100.0     0.4    
  # State vars at solution values
  continuous_state = 12
    descriptors = "fdene"  "hfact"  "coreradius"  "psepbqarmax"  "peakfactrad"  "etaech"  "feffcd"  "etaiso"  "boundl_18"  "alstroh"  "sig_tf_wp_max"  "triang"
    initial_state =       1.2  1.1907973921010135  0.7500000000000001  9.7  3.33  0.5  1.0  0.9  3.25  720000000.0  640000000.0  0.5

interface
  fork
    labeled
    analysis_drivers = 'python process_driver.py lt_sol.template'
    parameters_file = 'params.in'
    results_file    = 'responses.out'
    work_directory 
      named "runs/lt/run"
      directory_tag # Keep run dirs unique: required for parallelism
      link_files = "../run_dir_template/*"
      
      # Debug
      # directory_save # Keep individual run dirs
      # file_save # save Dakota .in and .out files

responses
  response_functions = 1
  descriptors "w"
  no_gradients
  no_hessians
  metadata "w_con_id"